# -*- coding: utf-8 -*-
"""Noah_FinalProjectDraft10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GzJlWs8U4XWUpVeuXOmvGyEx8r84pdf9
"""

import numpy as np
import pandas as pd
import os
import sys
import matplotlib.pyplot as plt
from scipy.io import loadmat
import h5py
from scipy import signal
from math import pi
from sklearn.preprocessing import MinMaxScaler,StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline

from google.colab import drive
drive.mount('/content/gdrive/')

"""### Data Import"""

# directory (dependent on what comp being used)
data_dir='./gdrive/My Drive/BMME890_MachineLearning/ProjectData'
# data_dir ='C:/Users/nmrubin/Desktop/BMME890MachineLearningProject/GoogleDrive/ProjectData'

#import neutral position emg data
#loadmat can't work with matlab v 7.3, use h5py

#ordered by subjects 1-8
Subjects= ['LV','KH','XH','HS','RC','NR','YZ','AM']

EMG_dir = data_dir + '/EMGData/'
Force_dir = data_dir + '/ForceData/'
Spks_dir = data_dir + '/SpikeTrainsData/'
# print(Subjects)

#later -> go through all subjects
for i in range(len(Subjects)): #Subject
    Subj = Subjects[i]

#for now one subject
Subj = 'LV'

#Fs = 1000 Hz

ForceFs = 1000

with h5py.File(Force_dir + Subj + '_Forces.mat', 'r') as file:
    ForceNI = np.array(file['ForceNItrial'])
    ForceNItime = np.array(file['ForceNItime'])
    ForceNM = np.array(file['ForceNMtrial'])
    ForceNMtime = np.array(file['ForceNMtime'])
    ForceNR = np.array(file['ForceNRtrial'])
    ForceNRtime = np.array(file['ForceNRtime'])
    ForceNP = np.array(file['ForceNPtrial'])
    ForceNPtime = np.array(file['ForceNPtime'])
    
    ForcePI = np.array(file['ForcePItrial'])
    ForcePItime = np.array(file['ForcePItime'])
    ForcePM = np.array(file['ForcePMtrial'])
    ForcePMtime = np.array(file['ForcePMtime'])
    ForcePR = np.array(file['ForcePRtrial'])
    ForcePRtime = np.array(file['ForcePRtime'])
    ForcePP = np.array(file['ForcePPtrial'])
    ForcePPtime = np.array(file['ForcePPtime'])
    
    ForceSI = np.array(file['ForceSItrial'])
    ForceSItime = np.array(file['ForceSItime'])
    ForceSM = np.array(file['ForceSMtrial'])
    ForceSMtime = np.array(file['ForceSMtime'])
    ForceSR = np.array(file['ForceSRtrial'])
    ForceSRtime = np.array(file['ForceSRtime'])
    ForceSP = np.array(file['ForceSPtrial'])
    ForceSPtime = np.array(file['ForceSPtime'])

#load emg data
#careful, will take up ~4-8GB Memory

#NPS = Neutral/Pronated/Supinated
#IMRP = Index/Middle/Ring/Pinky

#pick sample rate for appropriate subject
if Subj == 'LV':
    EMGFs =  2.5621e+03
else:
    EMGFs =  2.0497e+03

with h5py.File(EMG_dir + Subj + '_EMGtrials.mat', 'r') as file:
    EMGNI = np.array(file['EMGNI'])
    EMGNM = np.array(file['EMGNM'])
    EMGNR = np.array(file['EMGNR'])
    EMGNP = np.array(file['EMGNP'])
    
    EMGPI = np.array(file['EMGPI'])
    EMGPM = np.array(file['EMGPM'])
    EMGPR = np.array(file['EMGPR'])
    EMGPP = np.array(file['EMGPP'])
    
    EMGSI = np.array(file['EMGSI'])
    EMGSM = np.array(file['EMGSM'])
    EMGSR = np.array(file['EMGSR'])
    EMGSP = np.array(file['EMGSP'])
    
#first repetition is skipped in case of user-adjustments at beginning of trial, cut off first 19 seconds of EMG data to match with force
cutind = int(np.floor(19*EMGFs))
EMGNI = EMGNI[:,cutind:]
EMGNM = EMGNM[:,cutind:]
EMGNR = EMGNR[:,cutind:]
EMGNP = EMGNP[:,cutind:]

EMGPI = EMGPI[:,cutind:]
EMGPM = EMGPM[:,cutind:]
EMGPR = EMGPR[:,cutind:]
EMGPP = EMGPP[:,cutind:]

EMGSI = EMGSI[:,cutind:]
EMGSM = EMGSM[:,cutind:]
EMGSR = EMGSR[:,cutind:]
EMGSP = EMGSP[:,cutind:]

#load spiketrain data

#SpkTrn_PostureDecomposed_PostureforTrial_Finger
#e.g. SpkTrnNSI = Neutral MUs, Supinated Trial, Index Finger
#e.g. SpkTrnPPM = Pronated MUs, Pronated Trial, Middle Finger

#compare Neutral MUs against MUs decomposed in Pronated/Supinated positions

with h5py.File(Spks_dir + Subj + '_MUsyncscompiled.mat', 'r') as file:
    
    SpkTrnNNI = np.array(file['SpkTrnNNI'])
    SpkTrnNNM = np.array(file['SpkTrnNNM'])
    SpkTrnNNR = np.array(file['SpkTrnNNR'])
    SpkTrnNNP = np.array(file['SpkTrnNNP'])
    
    SpkTrnNPI = np.array(file['SpkTrnNPI'])
    SpkTrnNPM = np.array(file['SpkTrnNPM'])
    SpkTrnNPR = np.array(file['SpkTrnNPR'])
    SpkTrnNPP = np.array(file['SpkTrnNPP'])
    
    SpkTrnNSI = np.array(file['SpkTrnNSI'])
    SpkTrnNSM = np.array(file['SpkTrnNSM'])
    SpkTrnNSR = np.array(file['SpkTrnNSR'])
    SpkTrnNSP = np.array(file['SpkTrnNSP'])
      
#     SpkTrnPNI = np.array(file['SpkTrnPNI'])
#     SpkTrnPNM = np.array(file['SpkTrnPNM'])
#     SpkTrnPNR = np.array(file['SpkTrnPNR'])
#     SpkTrnPNP = np.array(file['SpkTrnPNP'])
    
    SpkTrnPPI = np.array(file['SpkTrnPPI'])
    SpkTrnPPM = np.array(file['SpkTrnPPM'])
    SpkTrnPPR = np.array(file['SpkTrnPPR'])
    SpkTrnPPP = np.array(file['SpkTrnPPP'])
    
#     SpkTrnPSI = np.array(file['SpkTrnPSI'])
#     SpkTrnPSM = np.array(file['SpkTrnPSM'])
#     SpkTrnPSR = np.array(file['SpkTrnPSR'])
#     SpkTrnPSP = np.array(file['SpkTrnPSP'])

#     SpkTrnSNI = np.array(file['SpkTrnSNI'])
#     SpkTrnSNM = np.array(file['SpkTrnSNM'])
#     SpkTrnSNR = np.array(file['SpkTrnSNR'])
#     SpkTrnSNP = np.array(file['SpkTrnSNP'])
    
#     SpkTrnSPI = np.array(file['SpkTrnSPI'])
#     SpkTrnSPM = np.array(file['SpkTrnSPM'])
#     SpkTrnSPR = np.array(file['SpkTrnSPR'])
#     SpkTrnSPP = np.array(file['SpkTrnSPP'])
    
    SpkTrnSSI = np.array(file['SpkTrnSSI'])
    SpkTrnSSM = np.array(file['SpkTrnSSM'])
    SpkTrnSSR = np.array(file['SpkTrnSSR'])
    SpkTrnSSP = np.array(file['SpkTrnSSP'])

"""Note: After cutoff of EMG data it still has 1 extra index compared to SpkTrn, not sure why"""

print(np.shape(EMGNI))
print(np.shape(SpkTrnNNI))
print()
print(np.shape(EMGSM))
print(np.shape(SpkTrnSSM))
print(np.shape(SpkTrnNSM))

print('Time per trial (seconds)')
forcetime = np.size(ForceNP)/1000
spktime = np.shape(SpkTrnNNM)[1]/EMGFs
emgtime = np.shape(EMGPI)[1]/EMGFs
print('Force Time')
print(forcetime)
print('EMG Time')
print(emgtime)
print('SpkTrain Time')
print(spktime)

#check shape of data as it is now
print('Shape of data')
print('Force shape')
print(np.shape(ForceNI))
print('EMG shape')
print(np.shape(EMGNI))
print('Spike Train shape')
print(np.shape(SpkTrnNNI))

"""Get RMS for EMG signals of top 85 channels"""

#calculate average rms of top channels
#check this
def rms(matrix,axis):
  
  return np.sqrt(np.mean(matrix**2,axis=axis))

def rms_top_channels(matrix,topN):
  rmsmatrix=rms(matrix,1)
  channelInd=np.argpartition(rmsmatrix,-topN)[-topN:] 
  topchannels=np.mean(matrix[channelInd,:],axis=0)
  return topchannels

#get mean rms values of top 85 channels
NIrms=rms_top_channels(EMGNI,85)
NMrms=rms_top_channels(EMGNM,85)
NRrms=rms_top_channels(EMGNR,85)
NPrms=rms_top_channels(EMGNP,85)

PIrms=rms_top_channels(EMGPI,85)
PMrms=rms_top_channels(EMGPM,85)
PRrms=rms_top_channels(EMGPR,85)
PPrms=rms_top_channels(EMGPP,85)

SIrms=rms_top_channels(EMGSI,85)
SMrms=rms_top_channels(EMGSM,85)
SRrms=rms_top_channels(EMGSR,85)
SPrms=rms_top_channels(EMGSP,85)
#remove number of steps corresponding to extra time from emg data
# NIrms=NIchannels[:,int((len(NIrms[0])*1/EMGFs-len(ForceNI)*1/1000)*EMGFs):]
# NMrms=NIchannels[:,int((len(NMrms[0])*1/EMGFs-len(ForceNM)*1/1000)*EMGFs):]

#calculate a rolling window, 500ms long incremented by 100ms
#need to have windows of the average times as well (i.e. [.25, .35, .45...etc])
def rolling_windows(signal,window,step,frequency):
    
    #check which dimension is time/longer
    dim = np.argmax(np.shape(signal))
    
    windowSize=round(window*frequency)  
    stepSize=round(step*frequency)
    windows=[]
    windtimes = []
    for start in np.arange(0,signal.shape[dim]-windowSize+1,stepSize):
        
        #track window times
        if start == 0: #start at half a window
            windtimes.append(window/2)
        else: #increment by one window step size
            windtimes.append(windtimes[-1] + step)
        
        end=start+windowSize
        
        if len(np.shape(signal)) == 1: #if column vector
            windows.append(np.array(signal[start:end]))
        else:
            if dim == 0:
                windows.append(np.array(signal[start:end,:])) #go down rows
            else:
                windows.append(np.array(signal[:,start:end])) #go down column
    
    return windows,windtimes

#get indices of the windows needed for cross-validation
def crossvalidataprep(windtimes):
    windtimes = np.array(windtimes)
    crossvalsectind = []
    Crossvaltimes = [0,26,49,72,95,118,141] #seconds, timing cutoffs for each repetition (7 total, last 141-end)
    for i in range(len(Crossvaltimes)):
        if i == 0:
            crossvalsectind.append(0)
        else:
            tmp = windtimes - Crossvaltimes[i]
            crossvalsectind.append(np.argmin(np.abs(tmp)))          
    return crossvalsectind
#crossvalsectind is vector of 6 values
#section 1 = crossvalind[0] thru crossvalidind[1]-1
#section 7 = crossvalind[end] to last window in signal

#feature generation, compute new features and combine into dataframe

def extrafeatures(RMSEMGwind):
    zc = []
    ssc = []
    std = []
    maxamp = []

    for window in RMSEMGwind:

        #zero crossings
        zc.append(len(np.ravel(np.diff(np.sign(window))).nonzero()[0]))

        #ssc
        dev2=np.gradient(np.gradient(window))
        ssc.append(len(np.ravel(np.diff(np.sign(dev2))).nonzero()[0]))

        #max amplitude
        maxamp.append(np.max(window))

        #std deviation
        std.append(np.std(window))
        
        extrafeat = pd.DataFrame({'Zero-Cross': zc,'SSC': ssc, 'STD' : std, 'Maxamp': maxamp})
        
    return extrafeat

#Calculate firing rate of Spike Train across windows (1603 windows)
def FRwind(SpkTrn,WinLength,StepLength,EMGFs):
# SpkTrn = SpkTrnNNI
# dim = np.argmax(np.shape(signal))
    emglength = np.shape(SpkTrn)[1]
    WinLength = 0.5 #seconds
    StepLength = 0.1
    WinNumSample = int(EMGFs*WinLength)
    StepNumSample = int(EMGFs*StepLength)
    TimeMax = emglength/EMGFs
    TimeWin = np.arange(WinNumSample/EMGFs,TimeMax,StepNumSample/EMGFs,dtype='float')
    timeemg = np.arange(1/EMGFs,(1+emglength)/EMGFs,1/EMGFs,dtype='float')

    FR = []

    MUs = np.shape(SpkTrn)[0]

    for i in range(1+np.size(TimeWin)):
        if i == 0: #first window
            windstartind = 0
            windstopind = np.argmin(abs(timeemg - WinLength))

        else: #move window forward
            windstartind = windstartind + StepNumSample + 1
            windstopind = windstopind + StepNumSample + 1

        if windstopind > emglength: #if next window goes past length of EMG, cut it off early
            windstopind = emglength
            WinLength = (windstopind - windstartind + 1)/EMGFs #last window time for new FR computation

        #compute firing rate in window
        FRtemp = np.zeros((1,MUs))
        for mu in range(MUs):
            FRtemp[0,mu] = np.count_nonzero(SpkTrn[mu,windstartind:windstopind])/WinLength

        FR.append((np.mean(FRtemp))) #get average firing rate

        if windstopind == emglength: #end of that was last window
            break
    return FR

#get firing rate windows
WinLength = .5
StepLength = .1
FRwindNNI = FRwind(SpkTrnNNI,WinLength,StepLength,EMGFs)
FRwindNNM = FRwind(SpkTrnNNM,WinLength,StepLength,EMGFs)
FRwindNNR = FRwind(SpkTrnNNR,WinLength,StepLength,EMGFs)
FRwindNNP = FRwind(SpkTrnNNP,WinLength,StepLength,EMGFs)

FRwindNSI = FRwind(SpkTrnNSI,WinLength,StepLength,EMGFs)
FRwindNSM = FRwind(SpkTrnNSM,WinLength,StepLength,EMGFs)
FRwindNSR = FRwind(SpkTrnNSR,WinLength,StepLength,EMGFs)
FRwindNSP = FRwind(SpkTrnNSP,WinLength,StepLength,EMGFs)

FRwindNPI = FRwind(SpkTrnNPI,WinLength,StepLength,EMGFs)
FRwindNPM = FRwind(SpkTrnNPM,WinLength,StepLength,EMGFs)
FRwindNPR = FRwind(SpkTrnNPR,WinLength,StepLength,EMGFs)
FRwindNPP = FRwind(SpkTrnNPP,WinLength,StepLength,EMGFs)

FRwindPPI = FRwind(SpkTrnNPI,WinLength,StepLength,EMGFs)
FRwindPPM = FRwind(SpkTrnNPM,WinLength,StepLength,EMGFs)
FRwindPPR = FRwind(SpkTrnNPR,WinLength,StepLength,EMGFs)
FRwindPPP = FRwind(SpkTrnPPP,WinLength,StepLength,EMGFs)

FRwindSSI = FRwind(SpkTrnSSI,WinLength,StepLength,EMGFs)
FRwindSSM = FRwind(SpkTrnSSM,WinLength,StepLength,EMGFs)
FRwindSSR = FRwind(SpkTrnSSR,WinLength,StepLength,EMGFs)
FRwindSSP = FRwind(SpkTrnSSP,WinLength,StepLength,EMGFs)

#Get emg windows
EMGNIwind,EMGNIwindtimes = rolling_windows(NIrms,.5,.1,EMGFs)
ForceNIwind,ForceNIwindtimes = rolling_windows(ForceNI,.5,.1,1000)
EMGNMwind,EMGNMwindtimes = rolling_windows(NMrms,.5,.1,EMGFs)
ForceNMwind,ForceNMwindtimes = rolling_windows(ForceNM,.5,.1,1000)
EMGNRwind,EMGNRwindtimes = rolling_windows(NRrms,.5,.1,EMGFs)
ForceNRwind,ForceNRwindtimes = rolling_windows(ForceNR,.5,.1,1000)
EMGNPwind,EMGNPwindtimes = rolling_windows(NPrms,.5,.1,EMGFs)
ForceNPwind,ForceNPwindtimes = rolling_windows(ForceNP,.5,.1,1000)
EMGPIwind,EMGPIwindtimes = rolling_windows(PIrms,.5,.1,EMGFs)
ForcePIwind,ForcePIwindtimes = rolling_windows(ForcePI,.5,.1,1000)
EMGPMwind,EMGPMwindtimes = rolling_windows(PMrms,.5,.1,EMGFs)
ForcePMwind,ForcePMwindtimes = rolling_windows(ForcePM,.5,.1,1000)
EMGPRwind,EMGPRwindtimes = rolling_windows(PRrms,.5,.1,EMGFs)
ForcePRwind,ForcePRwindtimes = rolling_windows(ForcePR,.5,.1,1000)
EMGPPwind,EMGPPwindtimes = rolling_windows(PPrms,.5,.1,EMGFs)
ForcePPwind,ForcePPwindtimes = rolling_windows(ForcePP,.5,.1,1000)
EMGSIwind,EMGSIwindtimes = rolling_windows(SIrms,.5,.1,EMGFs)
ForceSIwind,ForceSIwindtimes = rolling_windows(ForceSI,.5,.1,1000)
EMGSMwind,EMGSMwindtimes = rolling_windows(SMrms,.5,.1,EMGFs)
ForceSMwind,ForceSMwindtimes = rolling_windows(ForceSM,.5,.1,1000)
EMGSRwind,EMGSRwindtimes = rolling_windows(SRrms,.5,.1,EMGFs)
ForceSRwind,ForceSRwindtimes = rolling_windows(ForceSR,.5,.1,1000)
EMGSPwind,EMGSPwindtimes = rolling_windows(SPrms,.5,.1,EMGFs)
ForceSPwind,ForceSPwindtimes = rolling_windows(ForceSP,.5,.1,1000)
#should be same # windows for all of them, only need this once
crossvalsectind = crossvalidataprep(EMGNIwindtimes)

NIfeats=extrafeatures(EMGNIwind)
NMfeats=extrafeatures(EMGNMwind)
NRfeats=extrafeatures(EMGNRwind)
NPfeats=extrafeatures(EMGNPwind)

PIfeats=extrafeatures(EMGPIwind)
PMfeats=extrafeatures(EMGPMwind)
PRfeats=extrafeatures(EMGPRwind)
PPfeats=extrafeatures(EMGPPwind)

SIfeats=extrafeatures(EMGSIwind)
SMfeats=extrafeatures(EMGSMwind)
SRfeats=extrafeatures(EMGSRwind)
SPfeats=extrafeatures(EMGSPwind)

"""Cross-Correlate EMG/SpikeTrains & Force Data to account for delay"""

def crosscorr(a,b):
  lag=np.correlate(a-np.mean(a),b-np.mean(b),mode='full')
  lag=np.argmax(lag)-len(b)
  return lag

#rms
EMGNIrms=[np.mean(rms(np.expand_dims(window,1),1)) for window in EMGNIwind]
EMGNMrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGNMwind]
EMGNRrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGNRwind]
EMGNPrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGNPwind]
EMGNIrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGNIwind]

EMGPIrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGPIwind]
EMGPMrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGPMwind]
EMGPRrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGPRwind]
EMGPPrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGPPwind]

EMGSIrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGSIwind]
EMGSMrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGSMwind]
EMGSRrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGSRwind]
EMGSPrms=[np.mean(rms(np.expand_dims(window,0),1)) for window in EMGSPwind]

# get mean for force windows
ForceNImean=[np.mean(window,axis=1) for window in ForceNIwind]
ForceNMmean=[np.mean(window,axis=1) for window in ForceNMwind]
ForceNRmean=[np.mean(window,axis=1) for window in ForceNRwind]
ForceNPmean=[np.mean(window,axis=1) for window in ForceNPwind]

ForcePImean=[np.mean(window,axis=1) for window in ForcePIwind]
ForcePMmean=[np.mean(window,axis=1) for window in ForcePMwind]
ForcePRmean=[np.mean(window,axis=1) for window in ForcePRwind]
ForcePPmean=[np.mean(window,axis=1) for window in ForcePPwind]

ForceSImean=[np.mean(window,axis=1) for window in ForceSIwind]
ForceSMmean=[np.mean(window,axis=1) for window in ForceSMwind]
ForceSRmean=[np.mean(window,axis=1) for window in ForceSRwind]
ForceSPmean=[np.mean(window,axis=1) for window in ForceSPwind]

"""PLOT: before-after representative cross-correlation to fix lag-time between force & and emg/SpikeTrain data
Feature Extraction/Engineering:
"""

#remove extra dim
test=np.squeeze(ForceNImean)
delay=crosscorr(test,EMGNIrms[0:1605])
delay

#let's do cross correlation
tfrm_pipeline=Pipeline([('minmax_scaler', MinMaxScaler())])
fi=tfrm_pipeline.fit_transform(np.array(np.squeeze(ForceNImean)).reshape(-1,1))
ni=tfrm_pipeline.fit_transform(np.array(EMGNIrms).reshape(-1,1))

ftime=np.linspace(0,161,len(test))
etime=np.linspace(0,161.11,len(EMGNIrms))
plt.plot(ftime,fi,color='r')
plt.plot(etime,ni,color='b')

NIrms_shifted=np.roll(EMGNIrms,shift=-delay)
nis=tfrm_pipeline.fit_transform(NIrms_shifted.reshape(-1,1))
plt.plot(etime,nis,color='g')
plt.xlim(0,20)

# get cross correlations for all above
delayNI=crosscorr(np.squeeze(ForceNImean), EMGNIrms)
delayNM=crosscorr(np.squeeze(ForceNImean), EMGNMrms)
delayNR=crosscorr(np.squeeze(ForceNImean), EMGNRrms)
delayNP=crosscorr(np.squeeze(ForceNImean), EMGNPrms)

delayPI=crosscorr(np.squeeze(ForcePImean), EMGPIrms)
delayPM=crosscorr(np.squeeze(ForcePMmean), EMGPMrms)
delayPR=crosscorr(np.squeeze(ForcePRmean), EMGPRrms)
delayPP=crosscorr(np.squeeze(ForcePPmean), EMGPPrms)

delaySI=crosscorr(np.squeeze(ForceSImean), EMGSIrms)
delaySM=crosscorr(np.squeeze(ForceSMmean), EMGSMrms)
delaySR=crosscorr(np.squeeze(ForceSRmean), EMGSRrms)
delaySP=crosscorr(np.squeeze(ForceSPmean), EMGSPrms)

#Trim the EMG-Time-Domain Features
def trim_delay(delay,data,rms,force,finaltrim):
  trimmedData=pd.DataFrame()
  for feature in data.columns:
    featdata=np.roll(data[feature],shift=abs(delay))
    featdata=featdata[abs(delay):]
    trimmedData[feature]=featdata
  
    rms = np.roll(rms,shift = abs(delay))
    force=np.squeeze(force)
    trimmedForce=force[abs(delay):]  
    trimmedRMS=rms[abs(delay):]
    trimmedData['rms']=trimmedRMS
    trimmedData=trimmedData.iloc[0:finaltrim]
    trimmedForce=trimmedForce[0:finaltrim]
    return trimmedData,trimmedForce

smallestLength=1602
EMGNItrim,ForceNItrim=trim_delay(delayNI,NIfeats,EMGNIrms,ForceNImean,smallestLength)
EMGNMtrim,ForceNMtrim=trim_delay(delayNM,NMfeats,EMGNMrms,ForceNMmean,smallestLength)
EMGNRtrim,ForceNRtrim=trim_delay(delayNR,NRfeats,EMGNRrms,ForceNRmean,smallestLength)
EMGNPtrim,ForceNPtrim=trim_delay(delayNP,NPfeats,EMGNPrms,ForceNPmean,smallestLength)

EMGPItrim,ForcePItrim=trim_delay(delayPI,PIfeats,EMGPIrms,ForcePImean,smallestLength)
EMGPMtrim,ForcePMtrim=trim_delay(delayPM,PMfeats,EMGPMrms,ForcePMmean,smallestLength)
EMGPRtrim,ForcePRtrim=trim_delay(delayPR,PRfeats,EMGPRrms,ForcePRmean,smallestLength)
EMGPPtrim,ForcePPtrim=trim_delay(delayPP,PPfeats,EMGPPrms,ForcePPmean,smallestLength)

EMGSItrim,ForceSItrim=trim_delay(delaySI,SIfeats,EMGSIrms,ForceSImean,smallestLength)
EMGSMtrim,ForceSMtrim=trim_delay(delaySM,SMfeats,EMGSMrms,ForceSMmean,smallestLength)
EMGSRtrim,ForceSRtrim=trim_delay(delaySR,SRfeats,EMGSRrms,ForceSRmean,smallestLength)
EMGSPtrim,ForceSPtrim=trim_delay(delaySP,SPfeats,EMGSPrms,ForceSPmean,smallestLength)

#get cross-correlation for MUs
MUdelayNNI = crosscorr(np.squeeze(ForceNImean), FRwindNNI)
MUdelayNNM = crosscorr(np.squeeze(ForceNMmean), FRwindNNM)
MUdelayNNR = crosscorr(np.squeeze(ForceNRmean), FRwindNNR)
MUdelayNNP = crosscorr(np.squeeze(ForceNPmean), FRwindNNP)

MUdelayNPI = crosscorr(np.squeeze(ForcePImean), FRwindNPI)
MUdelayNPM = crosscorr(np.squeeze(ForcePMmean), FRwindNPM)
MUdelayNPR = crosscorr(np.squeeze(ForcePRmean), FRwindNPR)
MUdelayNPP = crosscorr(np.squeeze(ForcePPmean), FRwindNPP)

MUdelayNSI = crosscorr(np.squeeze(ForceSImean), FRwindNSI)
MUdelayNSM = crosscorr(np.squeeze(ForceSMmean), FRwindNSM)
MUdelayNSR = crosscorr(np.squeeze(ForceSRmean), FRwindNSR)
MUdelayNSP = crosscorr(np.squeeze(ForceSPmean), FRwindNSP)

MUdelayPPI = crosscorr(np.squeeze(ForcePImean), FRwindPPI)
MUdelayPPM = crosscorr(np.squeeze(ForcePMmean), FRwindPPM)
MUdelayPPR = crosscorr(np.squeeze(ForcePRmean), FRwindPPR)
MUdelayPPP = crosscorr(np.squeeze(ForcePPmean), FRwindPPP)

MUdelaySSI = crosscorr(np.squeeze(ForceSImean), FRwindSSI)
MUdelaySSM = crosscorr(np.squeeze(ForceSMmean), FRwindSSM)
MUdelaySSR = crosscorr(np.squeeze(ForceSRmean), FRwindSSR)
MUdelaySSP = crosscorr(np.squeeze(ForceSPmean), FRwindSSP)

#cross-correlate MU spike train and force data then shift and cut based on delay
def trim_MUdelay(delay,FR,force,finaltrim):
    
    FR = np.roll(FR,shift = abs(delay))
    force=np.squeeze(force)
    trimmedForce=force[abs(delay):]
    trimmedFR = FR[abs(delay):]
    # print('Frc')
    # print(len(trimmedForce))
    # print('FR')
    # print(len(trimmedFR))
    trimmedForce=trimmedForce[0:len(trimmedFR)]
    return trimmedFR,trimmedForce

#trim Firing rates to be same length as force after cross-correlation

#MU inputs
smallestLength = 1595
FRNNItrim,SpkFrcNNI = trim_MUdelay(MUdelayNNI,FRwindNNI,ForceNImean,smallestLength)
FRNNMtrim,SpkFrcNNM = trim_MUdelay(MUdelayNNM,FRwindNNM,ForceNMmean,smallestLength)
FRNNRtrim,SpkFrcNNR = trim_MUdelay(MUdelayNNR,FRwindNNR,ForceNRmean,smallestLength)
FRNNPtrim,SpkFrcNNP = trim_MUdelay(MUdelayNNP,FRwindNNP,ForceNPmean,smallestLength)

FRNPItrim,SpkFrcNPI = trim_MUdelay(MUdelayNPI,FRwindNPI,ForcePImean,smallestLength)
FRNPMtrim,SpkFrcNPM = trim_MUdelay(MUdelayNPM,FRwindNPM,ForcePMmean,smallestLength)
FRNPRtrim,SpkFrcNPR = trim_MUdelay(MUdelayNPR,FRwindNPR,ForcePRmean,smallestLength)
FRNPPtrim,SpkFrcNPP = trim_MUdelay(MUdelayNPP,FRwindNPP,ForcePPmean,smallestLength)

FRNSItrim,SpkFrcNSI = trim_MUdelay(MUdelayNSI,FRwindNSI,ForceSImean,smallestLength)
FRNSMtrim,SpkFrcNSM = trim_MUdelay(MUdelayNSM,FRwindNSM,ForceSMmean,smallestLength)
FRNSRtrim,SpkFrcNSR = trim_MUdelay(MUdelayNSR,FRwindNSR,ForceSRmean,smallestLength)
FRNSPtrim,SpkFrcNSP = trim_MUdelay(MUdelayNSP,FRwindNSP,ForceSPmean,smallestLength)

FRPPItrim,SpkFrcPPI = trim_MUdelay(MUdelayPPI,FRwindPPI,ForcePImean,smallestLength)
FRPPMtrim,SpkFrcPPM = trim_MUdelay(MUdelayPPM,FRwindPPM,ForcePMmean,smallestLength)
FRPPRtrim,SpkFrcPPR = trim_MUdelay(MUdelayPPR,FRwindPPR,ForcePRmean,smallestLength)
FRPPPtrim,SpkFrcPPP = trim_MUdelay(MUdelayPPP,FRwindPPP,ForcePPmean,smallestLength)

FRSSItrim,SpkFrcSSI = trim_MUdelay(MUdelaySSI,FRwindSSI,ForceSImean,smallestLength)
FRSSMtrim,SpkFrcSSM = trim_MUdelay(MUdelaySSM,FRwindSSM,ForceSMmean,smallestLength)
FRSSRtrim,SpkFrcSSR = trim_MUdelay(MUdelaySSR,FRwindSSR,ForceSRmean,smallestLength)
FRSSPtrim,SpkFrcSSP = trim_MUdelay(MUdelaySSP,FRwindSSP,ForceSPmean,smallestLength)

np.shape(SpkFrcSSI)

"""### Prep Input-Outputs"""

#EMG Time-Domain Features
#combine all fingers for one model (treat fingers as features)
X_EMGNeu = np.concatenate((EMGNItrim,EMGNMtrim),axis = 1)
X_EMGNeu = np.concatenate((X_EMGNeu,EMGNRtrim),axis = 1)
X_EMGNeu = np.concatenate((X_EMGNeu,EMGNPtrim),axis = 1)

X_EMGPro = np.concatenate((EMGPItrim,EMGPMtrim),axis = 1)
X_EMGPro = np.concatenate((X_EMGPro,EMGPRtrim),axis = 1)
X_EMGPro = np.concatenate((X_EMGPro,EMGPPtrim),axis = 1)

X_EMGSup = np.concatenate((EMGSItrim,EMGSMtrim),axis = 1)
X_EMGSup = np.concatenate((X_EMGSup,EMGSRtrim),axis = 1)
X_EMGSup = np.concatenate((X_EMGSup,EMGSPtrim),axis = 1)

#PCA Components
#Index

#Middle

#Ring

#Pinky

#combine all fingers for one model (treat fingers as features)

#MUs
FRNNI = np.reshape(FRNNItrim,[-1,1])
FRNNM = np.reshape(FRNNMtrim,[-1,1])
FRNNR = np.reshape(FRNNRtrim,[-1,1])
FRNNP = np.reshape(FRNNPtrim,[-1,1])

FRNPI = np.reshape(FRNPItrim,[-1,1])
FRNPM = np.reshape(FRNPMtrim,[-1,1])
FRNPR = np.reshape(FRNPRtrim,[-1,1])
FRNPP = np.reshape(FRNPPtrim,[-1,1])

FRNSI = np.reshape(FRNSItrim,[-1,1])
FRNSM = np.reshape(FRNSMtrim,[-1,1])
FRNSR = np.reshape(FRNSRtrim,[-1,1])
FRNSP = np.reshape(FRNSPtrim,[-1,1])

FRPPI = np.reshape(FRPPItrim,[-1,1])
FRPPM = np.reshape(FRPPMtrim,[-1,1])
FRPPR = np.reshape(FRPPRtrim,[-1,1])
FRPPP = np.reshape(FRPPPtrim,[-1,1])

FRSSI = np.reshape(FRSSItrim,[-1,1])
FRSSM = np.reshape(FRSSMtrim,[-1,1])
FRSSR = np.reshape(FRSSRtrim,[-1,1])
FRSSP = np.reshape(FRSSPtrim,[-1,1])

#index = zeros
#middle = ones
#encode for Index & Middle fingers together for spiketrains
#combine index/middle labels/forces



FRNNeu = np.concatenate((FRNNI,FRNNM))
indtmp = np.zeros([len(FRNNI),1])
midtmp = np.ones([len(FRNNM),1])
ohefing = np.concatenate((indtmp,midtmp))
X_FRNNeu = np.concatenate((FRNNeu,ohefing),axis=1)
y_SpksFrcNNeu = np.concatenate((SpkFrcNNI,SpkFrcNNM)).reshape(-1,1)
#
FRNPro = np.concatenate((FRNPI,FRNPM))
indtmp = np.zeros([len(FRNPI),1])
midtmp = np.ones([len(FRNPM),1])
ohefing = np.concatenate((indtmp,midtmp))
X_FRNPro = np.concatenate((FRNPro,ohefing),axis=1)
y_SpksFrcNPro = np.concatenate((SpkFrcNPI,SpkFrcNPM)).reshape(-1,1)
#
FRNSup = np.concatenate((FRNSI,FRNSM))
indtmp = np.zeros([len(FRNSI),1])
midtmp = np.ones([len(FRNSM),1])
ohefing = np.concatenate((indtmp,midtmp))
X_FRNSup = np.concatenate((FRNSup,ohefing),axis=1)
y_SpksFrcNSup = np.concatenate((SpkFrcNSI,SpkFrcNSM)).reshape(-1,1)
#
FRPPro = np.concatenate((FRPPI,FRPPM))
indtmp = np.zeros([len(FRPPI),1])
midtmp = np.ones([len(FRPPM),1])
ohefing = np.concatenate((indtmp,midtmp))
X_FRPPro = np.concatenate((FRPPro,ohefing),axis=1)
y_SpksFrcPPro = np.concatenate((SpkFrcPPI,SpkFrcPPM)).reshape(-1,1)
#
FRSSup = np.concatenate((FRSSI,FRSSM))
indtmp = np.zeros([len(FRSSI),1])
midtmp = np.ones([len(FRSSM),1])
ohefing = np.concatenate((indtmp,midtmp))
X_FRSSup = np.concatenate((FRSSup,ohefing),axis=1)
y_SpksFrcSSup = np.concatenate((SpkFrcSSI,SpkFrcSSM)).reshape(-1,1)

# X_FRNNeu
# X_FRNPro
# X_FRNSup
# X_FRPPro
# X_FRSSup
# y_SpksFrcNNeu
# y_SpksFrcNPro
# y_SpksFrcNSup
# y_SpksFrcPPro
# y_SpksFrcSSup

#Force
#reshape vectors to concatenate
FrcNI = np.reshape(ForceNItrim,[-1,1])
FrcNM = np.reshape(ForceNMtrim,[-1,1])
FrcNR = np.reshape(ForceNRtrim,[-1,1])
FrcNP = np.reshape(ForceNPtrim,[-1,1])

FrcPI = np.reshape(ForceSItrim,[-1,1])
FrcPM = np.reshape(ForceSMtrim,[-1,1])
FrcPR = np.reshape(ForceSRtrim,[-1,1])
FrcPP = np.reshape(ForceSPtrim,[-1,1])

FrcSI = np.reshape(ForceSItrim,[-1,1])
FrcSM = np.reshape(ForceSMtrim,[-1,1])
FrcSR = np.reshape(ForceSRtrim,[-1,1])
FrcSP = np.reshape(ForceSPtrim,[-1,1])

#combine all fingers for one model (treat fingers as features)
y_Neu = np.concatenate((FrcNI,FrcNM),axis=1)
y_Neu = np.concatenate((y_Neu,FrcNR),axis=1)
y_Neu = np.concatenate((y_Neu,FrcNP),axis=1)

y_Pro = np.concatenate((FrcPI,FrcPM),axis=1)
y_Pro = np.concatenate((y_Pro,FrcPR),axis=1)
y_Pro = np.concatenate((y_Pro,FrcPP),axis=1)

y_Sup = np.concatenate((FrcSI,FrcSM),axis=1)
y_Sup = np.concatenate((y_Sup,FrcSR),axis=1)
y_Sup = np.concatenate((y_Sup,FrcSP),axis=1)

#Sang
# Models to test:
# X_EMGNeu, y_Neu
# X_EMGNeu, y_Pro
# X_EMGNeu, y_Sup
# X_EMGPro, y_Pro
# X_EMGSup, y_Sup

"""PLOT: View correlations of features, remove some/etc.
Regression of Features to Force for Prediction:
"""

#Linear Regression of Firing Rates
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import TimeSeriesSplit,cross_val_score,cross_validate

lin_reg = LinearRegression()
#for each subject
tscv = TimeSeriesSplit(n_splits=8)
# NNIscores = cross_validate(lin_reg, FRNNItrain,FrcNItrain, cv=tscv, scoring=('r2','neg_mean_squared_error'))

def evaluateMUFR(X_FR,y_SpkFrc,lin_reg)
  scores = cross_validate(lin_reg, FRNNI,SpkFrcNNI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
  munniR2 = np.mean(scores['test_r2'])
  munniRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# scores = cross_validate(lin_reg, FRNNI,SpkFrcNNI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munniR2 = np.mean(scores['test_r2'])
# munniRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNNM,SpkFrcNNM, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munnmR2 = scores['test_r2']
# munnmRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNNR,SpkFrcNNR, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munnrR2 = np.mean(scores['test_r2'])
# munnrRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNNP,SpkFrcNNP, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munnpR2 = np.mean(scores['test_r2'])
# munnpRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# scores = cross_validate(lin_reg, FRNPI,SpkFrcNPI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munpiR2 = np.mean(scores['test_r2'])
# munpiRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNPM,SpkFrcNPM, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munpmR2 = np.mean(scores['test_r2'])
# munpmRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNPR,SpkFrcNPR, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munprR2 = np.mean(scores['test_r2'])
# munprRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNPP,SpkFrcNPP, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munppR2 = np.mean(scores['test_r2'])
# munppRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# scores = cross_validate(lin_reg, FRNSI,SpkFrcNSI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munsiR2 = np.mean(scores['test_r2'])
# munsiRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNSM,SpkFrcNSM, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munsmR2 = np.mean(scores['test_r2'])
# munsmRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNSR,SpkFrcNSR, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munsrR2 = np.mean(scores['test_r2'])
# munsrRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRNSP,SpkFrcNSP, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# munspR2 = np.mean(scores['test_r2'])
# munspRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# scores = cross_validate(lin_reg, FRPPI,SpkFrcPPI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# muppiR2 = np.mean(scores['test_r2'])
# muppiRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRPPM,SpkFrcPPM, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# muppmR2 = np.mean(scores['test_r2'])
# muppmRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRPPR,SpkFrcPPR, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# mupprR2 = np.mean(scores['test_r2'])
# mupprRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRPPP,SpkFrcPPP, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# mupppR2 = np.mean(scores['test_r2'])
# mupppRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# scores = cross_validate(lin_reg, FRSSI,SpkFrcSSI, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# mussiR2 = np.mean(scores['test_r2'])
# mussiRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRSSM,SpkFrcSSM, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# mussmR2 = np.mean(scores['test_r2'])
# mussmRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRSSR,SpkFrcSSR, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# mussrR2 = np.mean(scores['test_r2'])
# mussrRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))
# scores = cross_validate(lin_reg, FRSSP,SpkFrcSSP, cv=tscv, scoring=('r2','neg_mean_squared_error'))
# musspR2 = np.mean(scores['test_r2'])
# musspRMSE = np.mean(np.sqrt(-scores['test_neg_mean_squared_error']))

# ICA: already done
# PCA + K-Means
# EMG Time-Domain & Frequency Domain Features:
#     RMS
#     zero-crossings
# ANN???

# For MUs:
#     Linear (try separate groups/coefficients for each MU, but also try using average Firing rate as one coefficient, what my post-doc has used)
# For EMG:
#     Linear
#     ANN
#     SVM
#     ElasticNet
#     Lasso
#     Random-Forest
#     Kernel-Reaching
#     Gradient-Boost
#     SGD Regressor

# from sklearn.model_selection import cross_val_score,train_test_split,cross_validate
# from sklearn import tree,linear_model,svm
# from sklearn.preprocessing import MinMaxScaler,StandardScaler, OneHotEncoder
# from sklearn.pipeline import Pipeline
# from sklearn.metrics import r2_score
# import scipy.fftpack as syfp
# from sklearn.compose import ColumnTransformer

# Rather than table maybe bar chart:

# 4 Bar charts (one for each finger):
#     two sets of bars (R^2 + RMSE)
#         one bar for each model:
#             plot Mean +/- SD/SE across subjects
# sort by ranking or model??
# Use Linear for MUs, Linear + other top regressor model for comparison
# Further Analysis
# Comparisons repeated-measure ANOVA?
# Compare R^2/RMSE of MU regression against EMG
# Compare MUs regression of Neutral vs MUs decomposed in other posture

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import math  

R2 = r2_score(y_true, y_pred)
MSE = mean_squared_error(y_true, y_pred)

df['R^2'] = r2_score
df['RMSE'] = match.sqrt(MSE)

from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
import sklearn
import tensorflow as tf
from tensorflow import keras

## regressors set - up
KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))
ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))
GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, 
                                   loss='huber', random_state =5)
XGBoost = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)
LightgBoost = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

# stack set-up
from mlxtend.regressor import StackingCVRegressor
stack_gen = StackingCVRegressor(regressors=(KRR, lasso, ENet, GBoost,XGBoost,LightgBoost),
                                meta_regressor=XGBoost,
                                use_features_in_secondary=True)

# SELU model

modelS = keras.models.Sequential()

modelS.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for n_hidden in range(5):
    modelS.add(keras.layers.Dense(50, activation="selu",kernel_initializer="lecun_normal"))
modelS.add(keras.layers.Dense(10, activation="softmax"))


optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)
modelS.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])

# NN early stopping
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)
     
n_epochs = 100

import numpy as np
x_train = np.array([[1,3,1,0,7],[2,2,3,4,7],[3,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7]])
x_test = np.array([[1,3,1,0,7],[2,2,3,4,7],[3,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7],[4,2,3,4,7]])
y_train = np.array([[1],[2],[3],[4],[5],[6],[7]])
y_test = np.array([[1],[2],[3],[4],[5],[6],[7]])
print(np.shape(x))


# scale inputs
pixel_means = x_train.mean(axis=(0), keepdims=True)
pixel_stds = x_train.std(axis=(0), keepdims=True)
x_train_scaled = (x_train - pixel_means) / pixel_stds
x_test_scaled = (x_test - pixel_means) / pixel_stds

x_train.

y = y_train.tolist()
print(np.shape(y_train))

# fit
Fit1 = KRR.fit(x_train,y_train)
Fit2 = lasso.fit(x_train,y_train)
Fit3 = ENet.fit(x_train,y_train)
Fit4 = GBoost.fit(x_train,y_train)
Fit5 = XGBoost.fit(x_train,y_train)
Fit6 = LightgBoost.fit(x_train,y_train)
#stack_gen_t = stack_gen.fit(x_train,y_train)
#historyS = modelS.fit(x_train_scaled, y_train, epochs=n_epochs, validation_split=0.3,
#                    callbacks= [early_stopping_cb])

# evluate

EV1 = KRR.score(x_test, y_test)
EV2 = lasso.score(x_test, y_test)
EV3 = ENet.score(x_test, y_test)
EV4 = GBoost.score(x_test, y_test)
EV5 = XGBoost.score(x_test, y_test)
EV6 = LightgBoost.score(x_test, y_test)

#

#EV7 = stack_gen.evaluate(x_test_scaled, y_test,verbose=2)
#EV8 = modelS.evaluate(x_test_scaled, y_test,verbose=2)

# predict
#p1 = KRR.predict(test.values)
#p2 = lasso.predict(test.values)
#p3 = ENet.predict(test.values)
#p4 = GBoost.predict(test.values)
#p5 = XGBoost.predict(test.values)
#p6 = LightgBoost.predict(test.values)
#p7 = stack_gen.predict(test.values)
#p8 = modelS.predict(test.values)
# fit
Fit1 = KRR.fit(x_train,y_train)
Fit2 = lasso.fit(x_train,y_train)
Fit3 = ENet.fit(x_train,y_train)
Fit4 = GBoost.fit(x_train,y_train)
Fit5 = XGBoost.fit(x_train,y_train)
Fit6 = LightgBoost.fit(x_train,y_train)
#stack_gen_t = stack_gen.fit(x_train,y_train)
#historyS = modelS.fit(x_train_scaled, y_train, epochs=n_epochs, validation_split=0.3,
#                    callbacks= [early_stopping_cb])

# evluate

EV1 = KRR.score(x_test, y_test)
EV2 = lasso.score(x_test, y_test)
EV3 = ENet.score(x_test, y_test)
EV4 = GBoost.score(x_test, y_test)
EV5 = XGBoost.score(x_test, y_test)
EV6 = LightgBoost.score(x_test, y_test)

#

#EV7 = stack_gen.evaluate(x_test_scaled, y_test,verbose=2)
#EV8 = modelS.evaluate(x_test_scaled, y_test,verbose=2)

# predict
#p1 = KRR.predict(test.values)
#p2 = lasso.predict(test.values)
#p3 = ENet.predict(test.values)
#p4 = GBoost.predict(test.values)
#p5 = XGBoost.predict(test.values)
#p6 = LightgBoost.predict(test.values)
#p7 = stack_gen.predict(test.values)
#p8 = modelS.predict(test.values)

import pandas as pd 
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import math  
import scipy.stats as stats
from statsmodels.stats.multicomp import (pairwise_tukeyhsd,MultiComparison)

data = [['KernelRidge', EV1], ['Lasso', EV2], ['ElasticNet', EV3], ['GradientBoost', EV4], ['XGB', EV5], ['LGB', EV6]] 
  
df = pd.DataFrame(data, columns = ['Regressor', 'R2']) 

# R square calculation
#r2_score(y_true, y_pred)
# MSE calculation   
MSE = mean_squared_error(y_train, y_test)

# insert to dataframe
# df['R^2'] = r2_score
#df['RMSE'] = math.sqrt(mean_squared_error) 
df['RMSE'] = math.sqrt(MSE) 

# One Way ANOVA
#stats.f_oneway(tillamook, newport, petersburg, magadan, tvarminne)

## Tukey's multicomparison
# Set up the data for comparison (creates a specialised object)
#MultiComp = MultiComparison(stacked_data['result'],stacked_data['treatment'])

#print(MultiComp.tukeyhsd().summary())

df